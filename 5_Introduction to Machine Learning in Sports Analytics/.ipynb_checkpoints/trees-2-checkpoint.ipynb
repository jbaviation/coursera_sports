{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00001-e2ac1ea7-7831-4cc3-a8de-446a21ab685a",
    "deepnote_cell_type": "markdown",
    "etc_hash": "41b20507751f3db2caa5f6d5d153fb656cfe9b298521ad2d49a23149c32d1c59",
    "tags": []
   },
   "source": [
    "# A Multiclass Tree Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously we just looked at the two kinds of pitches, change ups and fastballs, and two different features, effective speed and spin rate. But we have both more features and more classes of pitches. Trees are flexible enough to work with any number of classes and features, just like support vector machines. So let's dig into this data in more detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00001-f4a438d1-fce5-4351-8eaf-8c52f9b4e359",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "etc_hash": "52b29eaa97d30f5a6f36c9eeef5b5b528fdcb52f6bc2a2c437eb6626be14a9e3",
    "execution_millis": 1480,
    "execution_start": 1621778384611,
    "source_hash": "82d04f3b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's go back to the full pitching data\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import zipfile\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "\n",
    "filename=\"assets/baseball_svm_data.zip\"\n",
    "df=pd.read_csv(zipfile.ZipFile(filename).open(\"reg_Sep2019.csv\"))\n",
    "color_list=list(mcolors.TABLEAU_COLORS.keys())\n",
    "df=df.groupby(\"pitch_type\").apply(lambda x: x.assign(color=color_list.pop())).reset_index(drop=True)\n",
    "df.plot.scatter(\"effective_speed\", \"release_spin_rate\", \n",
    "                s=1, figsize=(10, 4), title=\"Release Spin vs. Effective Speed\",\n",
    "                color=df[\"color\"], legend=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "etc_hash": "81cf17788c29ad0d926aa10ee6f9fcea4ffc45a69058d2b3d71198f31f1df209",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Now, in our SVM work we looked a number of different pitch metrics and game details\n",
    "# Seems like a fair approach to see how trees handle this\n",
    "pitch_metrics=['release_spin_rate','release_extension','release_pos_y','release_pos_x','release_pos_z','effective_speed']\n",
    "player_metrics=['player_name']\n",
    "game_details=['outs_when_up','inning']\n",
    "df=df[[*pitch_metrics, *player_metrics, *game_details, \"pitch_type\"]]\n",
    "\n",
    "# Create a feature vector for training\n",
    "features=[*pitch_metrics, *player_metrics, *game_details]\n",
    "\n",
    "# Now let's drop where any of the pitches are nan\n",
    "df=df.dropna(subset=[\"pitch_type\"])\n",
    "\n",
    "# And we factorize our player names and our outcomes\n",
    "df['player_name']=df['player_name'].factorize()[0]\n",
    "df['pitch_type2']=df['pitch_type'].factorize()[0]\n",
    "\n",
    "# We shuffle the data in the DataFrame to eliminate any sorting\n",
    "df=df.sample(frac=1, random_state=1337).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, before we create the validation and the training sets lets talk about\n",
    "# what we actually have in our data. Let's look at the prevelance of each class\n",
    "# - each type of pitch - in our actual data.\n",
    "df.groupby([\"pitch_type\",\"pitch_type2\"]).apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember we have this one pitch, the Eephus, which we almost always never\n",
    "# see. I'm going to get rid of that and just acknowledge the limitation of\n",
    "# our model is that it won't be something we can predict\n",
    "df=df.drop(df[df[\"pitch_type2\"]==2].index)\n",
    "\n",
    "# Also, now that we are in this multiclass scenario I want to randomly\n",
    "# sample from our dataframe for the test set.\n",
    "df_pitches=df.sample(5000,random_state=1337)\n",
    "\n",
    "# And we'll make our validation set just everything not in the sample\n",
    "df_validation=df[~df.index.isin(df_pitches.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ok, now we just need to impute those missing values throughout\n",
    "df_pitches=df_pitches.fillna(df_pitches.mean())\n",
    "df_validation=df_validation.fillna(df_validation.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So none of that was new, but lets now take a look at how a few different tree parameters\n",
    "# might change the descision boundaries - and the accuracy - of our classification. And just\n",
    "# because we're using a new algorithm doesn't mean we can't use the same powerful techniques\n",
    "# we have seen previously, like cross validation. This is one of the beautiful aspects of\n",
    "# the sklearn architecture\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "# Now we reduce to just our two columns which contain the features we expect are predictive\n",
    "X=df_pitches[features]\n",
    "y=df_pitches[\"pitch_type2\"]\n",
    "\n",
    "clfs={}\n",
    "# Let's parameterize and fit our models\n",
    "clfs[\"dt_1\"]=DecisionTreeClassifier(max_depth=1, random_state=1337)\n",
    "clfs[\"dt_2\"]=DecisionTreeClassifier(max_depth=2, random_state=1337)\n",
    "clfs[\"dt_3\"]=DecisionTreeClassifier(max_depth=3, random_state=1337)\n",
    "clfs[\"dt_4\"]=DecisionTreeClassifier(max_depth=4, random_state=1337)\n",
    "clfs[\"dt_5\"]=DecisionTreeClassifier(max_depth=5, random_state=1337)\n",
    "clfs[\"dt_unbounded\"]=DecisionTreeClassifier(random_state=1337)\n",
    "\n",
    "# Now we'll print out the accuracy scores\n",
    "for label, model in clfs.items():\n",
    "    # First let's cross validate to get an unbiased sense of accuracy\n",
    "    results=cross_validate(model,df_pitches[features],df_pitches[\"pitch_type2\"],cv=5,scoring='accuracy')\n",
    "    cv_acc=np.mean(results['test_score'])\n",
    "    # Next let's actually fit the model and score it to our unseen data\n",
    "    val_acc=model.fit(X,y).score(df_validation[features],df_validation[\"pitch_type2\"])\n",
    "    # Now let's look at our results\n",
    "    text=f\"{label} cv_acc={cv_acc:.4f} val_acc={val_acc:.4f}\"\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, there's a lot to unpack here. Let's start with one of the positives - did you notice how fast that was? Amazing. The SVMs took what seemed forever to train, but here the trees whipped through those 5,000 entries like nothing. Ok, but speed is only one consideration, and usually it's not the main one. We see that our actual validation set accuracy is a bit lower than our cross validation accuracy. This isn't uncommon, but it's not so far off. Keep in mind this is one random sampling of the data for our training data. If you change (or remove) that random state parameter you'll get different results.\n",
    "\n",
    "We've talked previously about the issue with accuracy as a metric, and considering how unbalanced our dataset is it seems like this makes things even more confusing. Let's look at the confusion matrix for that last model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "# In this case I'm going to set normalize to true, since I'm mostly interested\n",
    "# in relative accuracy of the classes\n",
    "matrix=plot_confusion_matrix(clfs[\"dt_unbounded\"], \n",
    "                             df_validation[features],\n",
    "                             df_validation[\"pitch_type2\"], \n",
    "                             xticks_rotation='vertical', \n",
    "                             cmap='cividis', \n",
    "                             normalize='true')\n",
    "matrix.figure_.set_size_inches(12,12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, we see a decently strong diagonal line, with a few classes below 50% accuracy on a nine class scale. One place that looks a lot like our boxing data is class 6, which we tend to predict more as class four than class six. How much does this matter? Well, it depends on your use case for the model - if you go back and look at our list of pitches you'll see that class four is a four seam fastball and class six is a two seam fastball. So the pitches are different but not nearly as different as, say, any fastball and change ups.\n",
    "\n",
    "Another good one to consider here is is pitch 7, which should be a knuckle curve ball, which we only correctly predict about a third of the time. We regularly misclassify this as either a change up or a slider. Both change ups and sliders join knuckle curve balls as off speed balls, moving slower than fastballs, and it's clear the model we've built is picking up on this. "
   ]
  }
 ],
 "metadata": {
  "deepnote": {},
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "9e72876e-9f53-4a2a-952c-aa0d839c094c",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
