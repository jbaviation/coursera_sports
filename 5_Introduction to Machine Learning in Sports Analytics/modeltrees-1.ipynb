{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "etc_hash": "82f6cbff58e49be64ca39989fe65cd1369cbfbfa3e30737ca2252121e8b2b9a5"
   },
   "source": [
    "# Model Trees: The Hart Memorial Trophy\n",
    "In our previous examples we've written all of our code in a single notebook, but this is actually a pretty rare case. A more authentic workflow is that we put specific libraries or functions in files which are meaingful based on what the file does, then we just import them into the notebook we are building. We're going to do that here. Now, I can't show you all the ins and outs of building deployable python packages here, but we'll do a bit of separation of concerns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00001-ca130ab6-8210-40a2-9cc3-52f2b3dca41e",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "etc_hash": "6ca1384020deb0ade00a479a1172de112910151db82a3c7036b890f3a831e171",
    "execution_millis": 155,
    "execution_start": 1622494351344,
    "source_hash": "a1292d3",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's start by bringing in our data science imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# And add to that the import_ipynb package, which allows us to call another jupyter\n",
    "# notebook from this one\n",
    "import import_ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00002-dd0f2f3e-657b-46fb-aa39-5363f90d5327",
    "deepnote_cell_type": "markdown",
    "etc_hash": "919af74a528c2746798db4095dd78c14e63856cbf8ce234a5ba873ca7963e2b3",
    "tags": []
   },
   "source": [
    "Now, we need a bunch of different data for this analysis. I'm going to show you how we got the data we needed, but most of the this won't work directly on the Coursera system because it doesn't support web scraping. Regardless, this will be useful if you want to replicate the work on your own computer, and of course I've included the datafiles I have pulled down so you can work on the machine learning parts as part of the course.\n",
    "\n",
    "Let's start by getting information on players, and that's going to come from the NHL API directly. Let's create a new notebook for that called [nhl_api.ipynb](nhl_api.ipynb).\n",
    "\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00003-c0f400de-ed02-41e4-8112-0c158208f20c",
    "deepnote_cell_type": "markdown",
    "etc_hash": "02c68542638933a608a0e59e063ea64c3eb9675f658b51cd4b5df18a24923dca",
    "tags": []
   },
   "source": [
    "Good, now that we have official player stats ready to go from the NHL API we have to consider our Hart Memorial Trophy voting data. Now, the Hart votes are done by reporters who cover the NHL, and the voting procedure has stayed relatively constant from 1996 up until the 2020-2021 season. This last season, and the playoffs are actually going on right now while I'm filming, modified the number of reporters who could vote because of structural changes to the season due to covid. So, will the data from the 2020-2021 season be useful for next year? I don't know, it might be that the league changes the way the trophy is voted for next year too. What about this current analysis we're doing, would that be useful for the 2020-2021 season? I can't answer that either, since the Hart results haven't been shared yet! This would be a great place for you to extend this analysis though, and see what impact the covid year has on building models like this.\n",
    "\n",
    "Regardless, we need to get that Hart voting data. There are several places to get this, but the website hockey-reference.com is a great resource and makes this easy, so let's create another API called [hockey_reference_api.ipynb](hockey_reference_api.ipynb).\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "etc_hash": "42322264bd0b1151d4b83f718e51054f9ca7a1dd9c5fc541e53493d0c917c08c"
   },
   "source": [
    "With our data now in hand, let's start our analysis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00002-a86a4b89-e8ed-40cd-a5e4-5ff379e785d7",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "etc_hash": "9d44a5f4ec8c9249c223360e458224a2d70f856a8de27ba781cdf475060b7c0f",
    "execution_millis": 137,
    "execution_start": 1622494351502,
    "source_hash": "6c0c0919",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# If you want to get the data directly from the API, just run the following:\n",
    "# import hockey_reference_api\n",
    "# historical_hart_results()\n",
    "\n",
    "# This takes awhile! On the Coursera system I've cached the datafles, so you\n",
    "# can just load them for to start our analysese\n",
    "df_votes=pd.read_csv(\"assets/historical_hart_results.csv\")\n",
    "df_players=pd.read_csv(\"assets/historical_player_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00005-0e96ccf9-3780-4635-b28a-c16e50e75c2a",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "etc_hash": "0b194d32239f100e588effa0faa9402018b948eddc9da353e42def7dfdeec22d",
    "execution_millis": 2,
    "execution_start": 1622494351642,
    "source_hash": "b4ca5c37",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The first challenge we run into is one we saw in the previous module,\n",
    "# sometimes the data isn't quite as clean as we would like it. It's very\n",
    "# common in manual data systems to find alignment issues, like we did\n",
    "# with the spelling of the team the Montreal Canadiens. In this case, it\n",
    "# turns out that two players have difference preffered names, and we need\n",
    "# to align them\n",
    "df_votes.loc[df_votes[\"Player\"]=='Alex Steen','Player'] = 'Alexander Steen'\n",
    "df_votes.loc[df_votes[\"Player\"]=='Olaf Kolzig','Player'] = \"Olie Kolzig\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00006-2d7d5eb6-5597-4d86-9025-049ab940a566",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "etc_hash": "23abd47bc85f4eebc9ad5837bc496a795c55a7d3137f5ce2be95217ab4b4013c",
    "execution_millis": 1,
    "execution_start": 1622494351685,
    "source_hash": "a3fda955",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We want to predict the number of votes that a given player will get in a\n",
    "# year. Actually, we don't care about the number of votes per se, which is\n",
    "# good because as membership in the PHWA changes there are more or less votes\n",
    "# cast each year. Instead, we want to predict the ratio of the votes that\n",
    "# a given player will get in a season. We're going to call this the normalized\n",
    "# vote percentage. This will be our regression target\n",
    "df_votes[\"normalized_vote_pct\"]=df_votes[\"Votes\"] / \\\n",
    "    df_votes.groupby(\"season\")[\"Votes\"].transform(np.sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00007-5fdb160c-7747-4a5d-94c8-2576f5c67150",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "etc_hash": "f95711ebc83932eee209c4f704a6f05d91e3fc0a5e8b631e8d55087bffd1a8c1",
    "execution_millis": 231,
    "execution_start": 1622494351686,
    "source_hash": "b2bc4b98",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# There are some interesting statistics in the data on the amount of time\n",
    "# each player spent on the ice, on the ice during power plays, which is when\n",
    "# one team has a penalty and the other has more people on the ice, and so\n",
    "# forth. These are all strings, but we want to convert these into a single\n",
    "# integer value of total seconds so they can be used by our model better\n",
    "\n",
    "def convert_on_ice_time_to_seconds(x:str) -> int:\n",
    "    '''Conver the string in the format of mm:ss to an integer of seconds\n",
    "    :param x: the mm:ss string from the NHL api\n",
    "    :return: the total number of seconds\n",
    "    '''\n",
    "    y = x.split(\":\")\n",
    "    return int(y[0])*60 + int(y[1])\n",
    "\n",
    "# We just want to apply this to all of the columns which have an \"OnIce\"\n",
    "# component in their name, and if a person has no entry then we will fill\n",
    "# it with the dummy value of 0:0 which should convert nicely.\n",
    "on_ice_columns = [c for c in df_players.columns if \"OnIce\" in c]\n",
    "df_players.loc[:, on_ice_columns] = df_players[on_ice_columns].fillna(\"0:0\").applymap(convert_on_ice_time_to_seconds).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00008-f79dae0d-5401-452e-a008-9259e829b411",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "etc_hash": "3edc06f8f6fbeceb0c250ddf264996a491af7978a3a7ced487ba98fcfc764a10",
    "execution_millis": 204,
    "execution_start": 1622494351921,
    "source_hash": "e4316534",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Now, let's merge these two files together and have a look at our\n",
    "# data!\n",
    "df_full=pd.merge(df_players,df_votes, how=\"left\", left_on=[\"fullName\",\"season\"],right_on=[\"Player\",\"season\"])\n",
    "df_full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00009-588f2dcf-6e32-4624-b7b8-99365e5cc39a",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "etc_hash": "57dc6a4e8deecc2f714c6c3d510b7d35220c4f7bcfb68b01c6aa70966bc64727",
    "execution_millis": 23,
    "execution_start": 1622494352121,
    "source_hash": "c14335dc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ok, two quick bits of data cleaning based on our look. We need to fill in\n",
    "# missing values - I'll just set them to 0 - and we need to make sure anyone\n",
    "# who didn't get votes gets a 0 as their vote percentage. \n",
    "# Everyone who did not get a vote should be recognized as a 0% chance, so fillna on y\n",
    "df_full=df_full.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00010-7bc2b2c8-55e8-4a06-a542-c2935ffc3570",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "etc_hash": "7640cb7c7fd11a11b09c5106b2778088ffac2d21299ef07db44778b58f8477e9",
    "execution_millis": 28,
    "execution_start": 1622494352149,
    "source_hash": "79905eb4",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# As we explored the dataset one of the students pointed out that a lot of the\n",
    "# stats might be only relevant for some positions, or that a player position\n",
    "# might influence a given statistic. This is very common in machine learning, that\n",
    "# there is a lack of independence between features. For instance, you would expect\n",
    "# a forward to be in a better position to score a goal than, say, a player on\n",
    "# defense. In addition, some stats, like the save percentages, are only calculate\n",
    "# for goalies, and thus are by definition non-existant for other positions.\n",
    "\n",
    "# The position code is available in our data, let's explore it\n",
    "df_full.groupby(\"position_code\").apply(lambda grp: np.mean(grp[\"normalized_vote_pct\"])/len(grp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00011-e6c6892e-b8dd-4a13-9530-3ea81c466451",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "etc_hash": "b907dbf9118adfa27c863e4a1379a8b567c100648e732a3a53f32d4ba96cb84c",
    "execution_millis": 42,
    "execution_start": 1622494352175,
    "source_hash": "d4b77073",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The position information is not a numeric value though. One way we can incorporate\n",
    "# this into our model is to change these into dummy indicators - e.g. five different\n",
    "# features, one for each position, which are either a 0 or a 1 if the player isn't\n",
    "# or is playing that position. Pandas makes this easy!\n",
    "df_full = pd.get_dummies(df_full, columns=['position_code'])\n",
    "\n",
    "# And we can create out holdout and training datasets\n",
    "df_holdout=df_full[df_full[\"season\"]==20182019]\n",
    "df_full=df_full[(df_full[\"season\"]<20182019) & (df_full[\"season\"]>=20012002)]\n",
    "\n",
    "# Let's build up a list of features we want to use. Here I'm going to choose a number\n",
    "# which I think might be interesting, you can feel free to explore!\n",
    "features=['assists', 'gameWinningGoals', 'games', 'gamesStarted', 'goalAgainstAverage', \n",
    "'goals', 'goalsAgainst', 'overTimeGoals', 'plusMinus', 'points', 'powerPlayGoals', \n",
    "'powerPlayPoints', 'savePercentage', 'saves', 'shortHandedGoals', 'shortHandedPoints', \n",
    "'shots', 'shotsAgainst', 'wins', 'blocked', 'evenSaves', 'evenShots', 'evenStrengthSavePercentage', \n",
    "'faceOffPct', 'hits', 'powerPlaySavePercentage', 'powerPlaySaves', 'powerPlayShots', \n",
    "'shortHandedSavePercentage', 'shortHandedSaves', 'shortHandedShots', 'position_code_C', \n",
    "'position_code_D', 'position_code_G', 'position_code_L', 'position_code_R',\n",
    "'timeOnIce', 'timeOnIcePerGame', 'evenTimeOnIce', 'evenTimeOnIcePerGame', 'powerPlayTimeOnIce', \n",
    "'powerPlayTimeOnIcePerGame', 'shortHandedTimeOnIce', 'shortHandedTimeOnIcePerGame',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00013-6dc2bc06-a4d3-4e2e-838d-57395bcbfb2c",
    "deepnote_cell_type": "markdown",
    "etc_hash": "2f603c10b6d5b41aa413e7963eb7a76cf39deea9bdb1e11f03d0e236336cb8a0",
    "tags": []
   },
   "source": [
    "Ok, we're just about at the exciting part: building a model! Before we do though, take a look at the list of the features which are in our model. Which features do you think are most informative for predicting the Hart trophy winner? Number of goals? Amount of time on ice? Shots blocked?\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00013-35450185-b63b-4e2e-b3cf-f4b747805f7b",
    "deepnote_cell_type": "markdown",
    "etc_hash": "f80d8630bf058ac7d7d6fb5a9bc564c45791dc67cff38a1288244438d6b36e40",
    "tags": []
   },
   "source": [
    "I know I said we were going to build the model, but I have to put one more shout out up here! Despite being well known, there is no python implementation of the M5 algorithm in scikit learn! However, [Sylvain Marie](https://github.com/smarie) in the Analytics and Cloud Platforms group from Schneider Electric has coded up the algorithm, and has made it available as open source on github. Even better they're currently pursuing getting this added to scikit learn, so maybe in the future this model will be available directly to us in sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00001-523dd809-0901-436f-a953-25e4a2984f5f",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "etc_hash": "84ec8f6f7a90fac04423421c18993bd6b4fefd63962bc6f191f0582b91af03a3",
    "execution_millis": 7013,
    "execution_start": 1622494352218,
    "source_hash": "9e709e0c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# I've put Sylvain's code in a python file and put that in the Coursera platform\n",
    "# for you. We can bring it into our current interpretor scope using the %run\n",
    "# magic function\n",
    "%run m5p.py\n",
    "\n",
    "# Now we just build our model as we have previously! Here I'll set a max depth\n",
    "# of the tree and the minimum number of samples per leaf, but feel free to play\n",
    "# with the parameters.\n",
    "m5p=M5Prime(max_depth=6, min_samples_leaf=3, use_smoothing=False)\n",
    "\n",
    "# We create our X and y\n",
    "X_train=df_full[features].reset_index(drop=True)\n",
    "y_train=df_full['normalized_vote_pct'].reset_index(drop=True)\n",
    "\n",
    "# I'm also going to store our dataframe for a future lecture\n",
    "df_full[[*features,'normalized_vote_pct']].to_csv(\"assets/model_tree_data.csv\",index=False)\n",
    "df_holdout[[*features,'normalized_vote_pct','fullName']].to_csv(\"assets/model_tree_holdout_data.csv\",index=False)\n",
    "\n",
    "from sklearn.model_selection import cross_validate\n",
    "results = cross_validate(m5p, X_train, y_train, cv=10, scoring='r2')\n",
    "\n",
    "print(f\"The cv score results are {results['test_score']}\")\n",
    "print(f\"The average cv score results are {np.mean(results['test_score'])} with a standard deviation of {np.std(results['test_score'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00015-08e2dda0-4e05-4d68-8304-9dc1b2db2d76",
    "deepnote_cell_type": "markdown",
    "etc_hash": "5ee75bc4bb8e82952885c3ec2e5f5cc7697ca9a2f4577c38f1d2b50037534e45",
    "tags": []
   },
   "source": [
    "Well, that's quite the range of $R^2$ values! This suggests to me that there might be a temporal nature to the accuracy of our models, and that we shouldn't put too much stock in the predictive power we've currently got - the standard deviation is really high.\n",
    "\n",
    "If we wanted to put this model into practice, we would want to look at particular folds in our validation which are particularly bad and consider which features may have led the model astray. I think it would be useful to think about these hyperparameters I just arbitrarily chose -- should the max depth of the tree be limited to 6?\n",
    "\n",
    "We'll tackle this a bit more in the next lecture where we tune and inspect the model tree."
   ]
  }
 ],
 "metadata": {
  "deepnote": {},
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "9058556b-fc45-408b-b81c-2b12551634ca",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
