{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00000-37b58b30-04c7-41bf-8682-7514c05e6889",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "etc_hash": "656788344afeaf2628d9fe1ed17ddb4685ccbdfa076740db8e1c2194e1a08f78",
    "execution_millis": 2819,
    "execution_start": 1621330130182,
    "source_hash": "c5e7ae78",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# And the standard data science data manipulation imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Bring in a plotting library\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "observations=pd.read_csv(\"assets/observations.csv\", index_col=0)\n",
    "observations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00002-a177ac25-5d00-4127-b1ae-973ce5b8cf8a",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "etc_hash": "a5702b0266b04696a894762cf334ebd69808416d70b0628c93714796297d5319",
    "execution_millis": 261,
    "execution_start": 1621330134251,
    "source_hash": "c322702b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Perhaps the biggest question to ecological validity - that is, the validity of this\n",
    "# model for real world work, like winning our office pool bets - is how well does it work\n",
    "# over time? In our first modeling approach we broke our training and test sets into two\n",
    "# pieces, a roughly ~800/450 split. Don't we want to dominate our office colleagues right\n",
    "# from the first game of the season though? :)\n",
    "\n",
    "# Let's look at the accuracy of this model over time. The index of the observations DataFrame\n",
    "# is in the format of yyymmdd_time, but for the sake of the pool we probably want to break\n",
    "# this into daily observations. I'm going to use a regular expression to do this, but you\n",
    "# can look at the str.split module or writer iterative code as well\n",
    "observations[\"date\"]=observations.index.str.extract(r\"(?P<date>[\\d]*)(?:[\\w]*)\",expand=False)\n",
    "observations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00003-14986ef8-1df4-4831-8ba3-27bbcd892e91",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "etc_hash": "08a9a06e62d5b2e7edaf1345ec26855a6a45e6a3f0eecc7f5e2156756c6f1e53",
    "execution_millis": 17240,
    "execution_start": 1621330134430,
    "source_hash": "ca785052",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Now that we have observations by date, we can build predictive models on a day by day basis.\n",
    "# Essentially, for each date we want to build a model which considers all of the data which\n",
    "# comes before it season and use that to predict the outcomes for the given date.\n",
    "\n",
    "# We're going to use the pandas groupby() function along with the apply() function. Together,\n",
    "# these functions will create a small DataFrame for each date in our data and allow us to\n",
    "# apply a single function to that data. The result of our function should be some accuracy\n",
    "# value as to how well our model actually performed\n",
    "def model_by_date(date_observations, features_list):\n",
    "    '''Takes a DataFrame of observations and uses it as a testing set for a model trained\n",
    "    on all previous observations in the global variable observations.\n",
    "    :param date_observations: A small DataFrame from observations indicating games played\n",
    "    on an individual day\n",
    "    '''\n",
    "    # Let's get all of the data before this date. We can access this date using the name field\n",
    "    training_df=observations[observations[\"date\"]<date_observations.name]\n",
    "\n",
    "    # We can only apply supervised machine learning if we have data to learn from\n",
    "    if len(training_df)>0:\n",
    "        # Let's build our model as before, first cleaning up the missing values\n",
    "        training_df=training_df.fillna(training_df.mean())\n",
    "        testing_df=date_observations.fillna(training_df.mean())\n",
    "\n",
    "        # Now train a model\n",
    "        from sklearn.linear_model import LogisticRegression\n",
    "        features=training_df[features_list] #.drop(['outcome_categorical','date'], axis='columns')\n",
    "        target=training_df['outcome_categorical']\n",
    "        clf=LogisticRegression()\n",
    "        reg=clf.fit(features,target)\n",
    "\n",
    "        # And now lets evaluate its accuracy\n",
    "        from sklearn.metrics import accuracy_score\n",
    "        labels=testing_df['outcome_categorical']\n",
    "        predictions=reg.predict(testing_df[features_list])#.drop(['outcome_categorical','date'], axis='columns'))\n",
    "\n",
    "        # Let's return the accuracy, as well as some metrics about what we've done\n",
    "        retvals={}\n",
    "        retvals[\"accuracy\"]=accuracy_score(labels,predictions)\n",
    "        \n",
    "        # How many observations did we train on?\n",
    "        retvals[\"num_training\"]=len(training_df)\n",
    "        \n",
    "        # How many observations did we predict?\n",
    "        retvals[\"num_testing\"]=len(testing_df)\n",
    "\n",
    "        # What's the actual model we trained?\n",
    "        retvals[\"model\"]=reg\n",
    "\n",
    "        # What are the coeficients for the regression learned\n",
    "        retvals[\"coef\"]=dict(zip(features,reg.coef_[0]))\n",
    "\n",
    "        return pd.Series(retvals)\n",
    "    \n",
    "    # There is a base case of what to do with the first season of the game. In this case\n",
    "    # I'm just going to return some missing values, but of course you might want to\n",
    "    # consider this special case a time when you train only on the historical data from\n",
    "    # last season\n",
    "    return pd.Series({\"accuracy\":np.nan, \"num_training\":np.nan, \"num_testing\":np.nan, \"model\":None, \"coef\":None})\n",
    "\n",
    "# Let's use all the features we have\n",
    "features_list=set(observations.columns)-set(('outcome_categorical','date'))\n",
    "daily_models_df=pd.DataFrame(observations.groupby(\"date\").apply(model_by_date, features_list))\n",
    "daily_models_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00004-cdc286d3-1f4e-4205-b3a0-143301b5e672",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "etc_hash": "99a6d9557fc036d5ef5bed923ffe26f17a6b8bdbb55796ea29d92d3ed8f2b3ac",
    "execution_millis": 447,
    "execution_start": 1621330151694,
    "source_hash": "9539ba2f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Great! We've productionize this code a bit, or at least simulate a production deployment\n",
    "# Let's do a bit of analysis to see how our model performs - as a daily average - throughout\n",
    "# the time period of observations\n",
    "daily_models_df[[\"num_training\",\"accuracy\"]].plot(secondary_y=\"accuracy\", xlabel=\"Date\", legend=False, figsize=(10, 4))\n",
    "plt.gcf().legend(loc='upper center');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00005-93c365e0-e0e0-4db4-ad08-74b8e5146eb4",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "etc_hash": "c9ba3073ef0f627998e23163710fcd84cb248987f4b1bad53b17e8729a6c7420",
    "execution_millis": 37027,
    "execution_start": 1621330152174,
    "output_cleared": false,
    "source_hash": "59df6bd0",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# So this was a bit surprising to me. I thought we would find a regularly increasing model, growing\n",
    "# in accuracy as the number of training instances do. But while we see a bit of that in the begining,\n",
    "# it doesn't hold for long and the model seems random throughout the whole date range.\n",
    "\n",
    "# Generally, we would want to explore the model to understand which features might be performing\n",
    "# well. The most common approach would be to inspect the coefficients for the regression models being\n",
    "# produced. But given we have some buckets of features, we could also explore the accuracy as it \n",
    "# relates to these buckets. Let's give that a shot.\n",
    "\n",
    "# First, I'm going to silence a warning coming from sklearn with respect to convergence of our\n",
    "# logistic regression model. It's important if you are considering using these models, but given the\n",
    "# number of models we are creating it will add a lot of noise to the output\n",
    "from warnings import simplefilter\n",
    "simplefilter(action='ignore')\n",
    "\n",
    "# Let's build models with just the previously seen performance this season\n",
    "cur_season_model=pd.DataFrame(observations.groupby(\"date\").apply(model_by_date, ['home_lost', 'home_won', 'away_lost', 'away_won']))\n",
    "\n",
    "# Now let's build models with just the last season's information in it\n",
    "past_season_features=set(observations.columns)-set(('home_lost', 'home_won', 'away_lost', 'away_won', 'home_cap', 'away_cap', 'outcome_categorical','date'))\n",
    "past_season_model=pd.DataFrame(observations.groupby(\"date\").apply(model_by_date, past_season_features))\n",
    "\n",
    "# Now let's just look at the salary cap information\n",
    "salary_cap_model=pd.DataFrame(observations.groupby(\"date\").apply(model_by_date, ['home_cap', 'away_cap']))\n",
    "\n",
    "# Let's plot their accuracies\n",
    "fig, ax = plt.subplots(3, figsize=(10, 9))\n",
    "\n",
    "ax[0].set_title(\"Current Season Features\")\n",
    "cur_season_model[\"accuracy\"].plot(ax=ax[0])\n",
    "ax[1].set_title(\"Past Season Standings\")\n",
    "past_season_model[\"accuracy\"].plot(ax=ax[1])\n",
    "ax[2].set_title(\"Salary Cap\")\n",
    "salary_cap_model[\"accuracy\"].plot(ax=ax[2])\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00006-53636e2d-c437-498e-a6b0-5870d7bd3949",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "etc_hash": "3ffb7e5a1e64d46b2e02fdf05a5c017c6a6d11ccee3207e50dd93a158d9445c9",
    "execution_millis": 0,
    "execution_start": 1621330189152,
    "source_hash": "f52fd602",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# A question that's very hot in the machine learning literature these days\n",
    "# is the issue of bias in predictive models. Is the model biased when predicting\n",
    "# the outcomes of one or more groups? Generally, the focus is on societal\n",
    "# groups where such bias might reinforce inequities where we might not expect\n",
    "# or desire it. But the issue of bias can be generalized from there, to consider\n",
    "# the cases where the model performs poorly, and thus shouldn't be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00007-aa7012ea-a8ab-435e-b537-57869713332a",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "etc_hash": "adc03be1b60210e01593d366a6f7da19150bd68a53dfe4036679c7b7b65ad706",
    "execution_millis": 73740,
    "execution_start": 1621334928106,
    "source_hash": "d92a4961",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Actually, this dataset is interesting in that regard, one of our teams - \n",
    "# the Vegas Golden Knights - has a lot of missing information.\n",
    "# You'll recall that we imputed this as the average of all of the\n",
    "# other team's salary cap data. Is that reasonable? More generally, with\n",
    "# respect to the games the Vegas Golden Knights were playing in, do our predictive\n",
    "# models have poor accuracy?\n",
    "\n",
    "def model_by_game(row):\n",
    "    \"\"\"Returns the teams involved and the model accuracy for a given entry\n",
    "    in the observations DataFrame.\n",
    "    :param row: A single game from the global observations DataFrame\n",
    "    :param gametime: The time the game is played\n",
    "    :return: A dict in the form of {\"teams\":[\"home team name\",\"away team name\"],\n",
    "                                   \"correct_prediciton\":True}\n",
    "    \"\"\"\n",
    "    # Let's get all of the data from games before this one. In this case we will assume\n",
    "    # the game has ended and that we know all of the information from previous\n",
    "    # games\n",
    "    training_df=observations[observations.index<row.name]\n",
    "    features_list=set(training_df.drop(['outcome_categorical','date'], axis='columns').columns)\n",
    "\n",
    "    # We want to build a model for single games, but we run into a problem like\n",
    "    # we did with data before. This time it's a bit different, we need to make sure\n",
    "    # that we see at least two different labels to classify - one where the away team\n",
    "    # has won and one where the home team has won. Otherwise the classifer doesn't\n",
    "    # know how to label our data. The approach I'll take here is just to try and\n",
    "    # classify the data, but if there is an error I'll just send back empty data.\n",
    "    try:\n",
    "        # Now, we're specifically interested in the teams which played, but we scrubbed\n",
    "        # this information from our model! But it turns out we have a mapping between\n",
    "        # teams and this data, because we've captured the league ranking from last year,\n",
    "        # and that value is going to be an np.nan for the Golden Knights. Before we impute\n",
    "        # it, let's make sure we are going to send back these team identifiers       \n",
    "        retvals=[{\"team\":str(row[\"home_last_season_leagueRank\"])},{\"team\":str(row[\"away_last_season_leagueRank\"])}]\n",
    "\n",
    "        # Let's build our model as before, first cleaning up the missing values\n",
    "        training_df=training_df.fillna(training_df.mean())\n",
    "        testing_df=row.fillna(training_df.mean())\n",
    "\n",
    "        # Now train a model\n",
    "        from sklearn.linear_model import LogisticRegression\n",
    "        features=training_df[features_list]\n",
    "        target=training_df['outcome_categorical']\n",
    "        clf=LogisticRegression()\n",
    "        reg=clf.fit(features,target)\n",
    "\n",
    "        # And now lets evaluate its accuracy\n",
    "        from sklearn.metrics import accuracy_score\n",
    "        labels=testing_df['outcome_categorical']\n",
    "        predictions=reg.predict((testing_df[features_list].values,))\n",
    "\n",
    "        # Let's return the accuracy, as well as the team names\n",
    "        retvals[0][\"correct_prediction\"]=(labels==predictions[0])\n",
    "        retvals[1][\"correct_prediction\"]=(labels==predictions[0])\n",
    "        return pd.DataFrame(retvals)\n",
    "    except:\n",
    "        # An error of some kind, likely that the classifier hasn't seen enough data to run\n",
    "        return pd.DataFrame([{\"team\":None, \"correct_prediction\":None},{\"team\":None, \"correct_prediction\":None}])\n",
    "\n",
    "model_results=observations.apply(model_by_game, axis=\"columns\")\n",
    "\n",
    "# And this will be a series of nested results, so let's just collapse that to a\n",
    "# single dataframe\n",
    "df=pd.DataFrame()\n",
    "for i in range(len(model_results)):\n",
    "    df=df.append(model_results.iloc[i])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00008-eac7a0e7-f301-4ce4-a88e-9f953025c0d9",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "etc_hash": "8c80dd8b18bdaaef9dcbb0c1c7952d142d74c7ab9c0f094cf841701b0ce5e734",
    "execution_millis": 47,
    "execution_start": 1621335243747,
    "source_hash": "ebd867fe",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# All right, now that we have a dataframe which has our teams and whether they\n",
    "# have had a correct prediction or not, we can group by the team identifier and\n",
    "# apply an aggregation function to determine how many times the model was correct\n",
    "# for that given team. We can use the numpy count_nonzero function to do this,\n",
    "# as a true value is represented by a 1 in numpy, while a false value is represented\n",
    "# with a zero\n",
    "df.groupby(\"team\").agg({\"correct_prediction\":np.count_nonzero}).sort_values(\"correct_prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00009-61b00b4b-841e-458e-b345-a4dc23979887",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "etc_hash": "158111f6ef042ee84f3e7f5fcb9a52023bfd046f4812bd04c2a5e95d1f712ef2",
    "execution_millis": 15,
    "execution_start": 1621335577789,
    "source_hash": "d3cc3253",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ok! So we see that this model has some bias when making predictions, in particular it\n",
    "# is a poor predictor for matches where team 12 and 30 are in, and it's a better\n",
    "# predictor for games where team 19 and 9 are involved. Our np.nan which are the Vegas \n",
    "# Golden Knights, and is a pretty chance predictor. None of these are amazing values,\n",
    "# as we know, given that our model isn't so great, but this is one way to see if that\n",
    "# bias is well spread throughout the model. Keep in mind, each team plays roughly 84\n",
    "# games in this dataset.\n",
    "\n",
    "# So this of course begs the question, who is team 12 and 30, whom the model is most biased\n",
    "# against when it comes to accurate predictions? Let's look.\n",
    "pd.read_csv(\"assets/previous_season_standings.csv\").query('(leagueRank==12) or (leagueRank==30)')[\"team.name\"]"
   ]
  }
 ],
 "metadata": {
  "deepnote": {
   "is_reactive": false
  },
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "faabfd23-7640-4c7b-a7bc-afebdc3a2cc8",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
